{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3E0UFhEqYoKl"
      },
      "source": [
        "<h1 style=\"text-align: center;\">A Tutorial for (Probabilistic) Bayesian Neural Networks</h1>\n",
        "<p style=\"text-align: center;\"> A short version of <a href=\"https://keras.io/examples/keras_recipes/bayesian_neural_networks/\">Khalid Salama's blog post</a> with Son Hai Le's modifications </p>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Goal of this document**\n",
        "- Implementation of standard neural network ([SNN](http://www.deeplearningbook.org))\n",
        "- Implementation of 2 types of Bayesian neural network ([BNN](https://ieeexplore.ieee.org/document/9756596)):\n",
        "    * BNN: deal with [epistemic uncertainty](https://www.sciencedirect.com/science/article/abs/pii/S0167473008000556)\n",
        "    * Probabilistic BNN: deal with both [aleatoric uncertainty](https://www.sciencedirect.com/science/article/abs/pii/S0167473008000556) and [epistemic uncertainty](https://www.sciencedirect.com/science/article/abs/pii/S0167473008000556)\n",
        "\n",
        "\n",
        "**What are different compared to the original** [Khalid Salama's blog post](https://keras.io/examples/keras_recipes/bayesian_neural_networks)?\n",
        "- Make the document more succinct, remove unnecessary theory parts\n",
        "- Add an [exploratory data analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis)\n",
        "- Add codes to save and load the weights of the model\n",
        "- Add visualization of the predictions\n",
        "\n",
        "*Note: follow [hyperlinks]() to see more details*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj_VfGUlYoKp"
      },
      "source": [
        "## Python packages\n",
        " - [TensorFlow](https://www.tensorflow.org/) 2.3 or higher: for neural networks\n",
        " - [TensorFlow Probability](https://www.tensorflow.org/probability) (`pip install tensorflow-probability`): for Bayesian neural networks\n",
        " - [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/wine_quality) (`pip install tensorflow-datasets`): for dataset\n",
        " - [Pandas](https://pandas.pydata.org/) (`pip install pandas`): for data structures and data analysis\n",
        " - [SeaBorn](https://seaborn.pydata.org/) (`pip install seaborn`) and [matplotlib](https://matplotlib.org/) (`pip install matplotlib`): for data visualization\n",
        "\n",
        " TensorFlow 2.3 이상: 머신러닝 및 딥러닝을 위한 오픈소스 라이브러리로, 신경망 구축에 주로 사용됩니다.\n",
        "\n",
        "TensorFlow Probability (pip install tensorflow-probability): 베이지안 신경망 구현을 위한 확률적 계산 및 통계 모델링을 지원하는 TensorFlow 확장 라이브러리입니다.\n",
        "\n",
        "TensorFlow Datasets (pip install tensorflow-datasets): 다양한 머신러닝 데이터셋을 손쉽게 다운로드하고 사용할 수 있게 해주는 TensorFlow의 데이터셋 라이브러리입니다.\n",
        "\n",
        "Pandas (pip install pandas): 데이터 조작 및 분석을 위한 라이브러리로, 효율적인 데이터 구조를 제공하며 테이블 형태의 데이터를 다루는 데 유용합니다.\n",
        "\n",
        "SeaBorn (pip install seaborn): 데이터 시각화를 위한 파이썬 라이브러리로, 복잡한 통계 그래프를 쉽게 생성할 수 있습니다. Matplotlib 기반으로 작동합니다.\n",
        "\n",
        "Matplotlib (pip install matplotlib): 다양한 형태의 그래프와 차트를 생성하기 위한 파이썬 데이터 시각화 라이브러리로, 2D 및 3D 그래프를 그릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[1;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_probability\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
          ]
        }
      ],
      "source": [
        "# %matplotlib inline\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from IPython.display import HTML\n",
        "\n",
        "# 아래는 그래프등을 보여주기위한 주피터노트북 관련된 형식 설정(무시가능)\n",
        "HTML(\"\"\"\n",
        "<style>\n",
        "h1,h2,h3 {\n",
        "\tmargin: 1em 0 0.5em 0;\n",
        "\tfont-weight: 600;\n",
        "\tfont-family: 'Titillium Web', sans-serif;\n",
        "\tposition: relative;  \n",
        "\tfont-size: 36px;\n",
        "\tline-height: 40px;\n",
        "\tpadding: 15px 15px 15px 2.5%;\n",
        "\tcolor: #1E8449;\n",
        "\tbox-shadow: \n",
        "\t\tinset 0 0 0 1px rgba(246,38,100, 1), \n",
        "\t\tinset 0 0 5px rgba(246,3,100, 1),\n",
        "\t\tinset -285px 0 35px #D5F5E3;\n",
        "\tborder-radius: 0 10px 0 15px;\n",
        "\tbackground: #fff\n",
        "    \n",
        "}\n",
        "</style>\n",
        "\"\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LXM2Ja1pYoKr"
      },
      "source": [
        "## Dataset\n",
        "#### Feautures description\n",
        "The wine subset of [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset:\n",
        "- 4,898 examples\n",
        "- Input: 11 numerical physicochemical features of the wine\n",
        "    * Fixed acidity: the amount of tartaric acid in wine and (g/dm3)\n",
        "    * Volatile acidity: the amount of acetic acid in the wine (g/dm3)\n",
        "    * Citric acid: the amount of citric acid in the wine (g/dm3)\n",
        "    * Residual sugar: the amount of sugar remaining after fermentation (g/dm3)\n",
        "    * Chlorides: the amount of salt in the wine (g/dm3)\n",
        "    * Free sulfur dioxide: the amount of free sulfur dioxide in the wine (mg/dm3)\n",
        "    * Total sulfur dioxide: the amount of free and bound forms of sulfur dioxide (mg/dm3)\n",
        "    * Density: the density of the wine (g/cm3)\n",
        "    * pH: the pH of the wine\n",
        "    * Sulfates: the amount of sulfur dioxide bound to potassium and sodium (g/dm3)\n",
        "    * Alcohol: the alcohol content of the wine (% by volume)\n",
        "- Output: the quality of the wine, which is ranged from 1 to 10 (the higher, the better)\n",
        "\n",
        "=> **A regression task**: predict the wine quality score from the input features.\n",
        "\n",
        "\n",
        "#### 11가지의 사전 조건(산미, 당도 등등.. )을 가지고 quality 점수를 도출하는 모델 작성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the wine quality dataset\n",
        "data_tf = tfds.load('wine_quality',split='train')\n",
        "df = tfds.as_dataframe(data_tf)\n",
        "\n",
        "# Rename columns\n",
        "df.columns = [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\n",
        "              \"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\",\"quality\"]\n",
        "\n",
        "# Create a box plot for all numerical columns\n",
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Descriptive statistics\n",
        "#### 설명 통계(Descriptive statistics)는 데이터를 요약하고 설명하는 통계 기법입니다. 이를 통해 데이터의 중심 경향, 퍼짐 정도, 분포 형태 등의 특징을 쉽게 파악할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical Analysis \n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# datatype information:\n",
        "df.info()\n",
        "\n",
        "# 데이터 베이스 정보 불러오기(신경 안써도됨)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " #### Data preprocessing\n",
        "  #### 데이터 전처리 과정 데이터프레임(df) 내의 결측값을 시각화하기 위한 코드\n",
        "  #####  Seaborn 라이브러리의 heatmap 함수(매트랩 그래프 그리는거 비슷한 느낌)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check for missing values:\n",
        "\n",
        "# matplotlib 라이브러리를 사용하여 그림의 크기를 설정합니다. 여기서는 가로 10, 세로 5의 크기로 설정되어 있습니다.\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# Seaborn 라이브러리의 heatmap 함수를 사용하여 결측값을 표시하는 히트맵을 생성합니다. df.isnull()을 통해 데이터프레임 내의 결측값을 찾고, yticklabels=False로 설정하여 y축 레이블을 숨깁니다. cbar=True는 오른쪽에 컬러바를 표시하도록 설정하며, cmap='mako'를 사용하여 색상 팔레트를 지정합니다.\n",
        "sns.heatmap(df.isnull(),yticklabels=False,cbar=True,cmap='mako')\n",
        "\n",
        "# x축 레이블의 회전 각도를 설정합니다. 여기서는 45도로 회전하도록 설정되어 있습니다.\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# 데이터프레임 내의 결측값 개수를 각 열별로 합산하여 출력합니다.\n",
        "df.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exploratory data analysis (EDA)\n",
        "##### 탐색적 데이터 분석(Exploratory Data Analysis, EDA)은 데이터를 다양한 관점에서 살펴보고, 데이터의 구조, 패턴, 이상치, 변수간의 관계 등을 파악하는 과정입니다. EDA를 통해 데이터에 대한 이해를 높이고, 적절한 모델링 방법을 결정하거나 데이터 전처리를 수행할 수 있다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Count plot of quality variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seaborn 라이브러리의 막대형 합산 그래프\n",
        "sns.countplot(x='quality',data=df)  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Lets see whether our data has outliers or not:**\n",
        "- By the below plot: lots of outliers\n",
        "- However, won't remove them because they are not errors, they are just extreme values\n",
        "\n",
        "**데이터에 특이치가 있는지 여부를 확인합니다:**\n",
        "- 많은 평균값을 벗어나는 값들\n",
        "- 하지만 오류가 아니라 극단적인 유효한 값이기 때문에 무시하지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 박스 플롯 생성을 위한 서브플롯 그리드 설정\n",
        "fig, ax = plt.subplots(ncols=6, nrows=2, figsize=(20,10))\n",
        "\n",
        "# 박스 플롯을 그릴 서브플롯 인덱스 초기화\n",
        "index = 0\n",
        "\n",
        "# 서브플롯 배열을 1차원 배열로 변환\n",
        "ax = ax.flatten()\n",
        "\n",
        "# 데이터프레임의 각 열에 대해 반복하여 박스 플롯 생성\n",
        "for col, value in df.items():\n",
        "    # 해당 열의 박스 플롯 생성 및 서브플롯에 그리기\n",
        "    sns.boxplot(y=col, data=df, color='b', ax=ax[index])\n",
        "    \n",
        "    # 서브플롯 인덱스 증가\n",
        "    index += 1\n",
        "\n",
        "# 전체 그림 레이아웃 조정\n",
        "plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Find correlated columns from below figure**\n",
        "- Wine quality:\n",
        "    * Positively correlated with the fixed acidity (strongly)\n",
        "    * Negatively correlated with the residual sugar (strongly)\n",
        "- Residual sugar:\n",
        "    * Positively correlated with the density and sulfates (strongly)\n",
        "- Chlorides: \n",
        "    * Negatively correlated with the total sulfur dioxide (strongly)\n",
        "- Free sulfur dioxide:\n",
        "    * Positively correlated with the sulphates (strongly)\n",
        "- Density:\n",
        "    * Positively correlated with the sulphates (strongly)\n",
        "\n",
        "**아래 그림에서 상관된 열 찾기**\n",
        "- 와인 품질:\n",
        "    * 고정된 산도와 양의 상관관계(강력)\n",
        "    * 잔류 당과 음의 상관관계(강력)\n",
        "- 잔류 설탕:\n",
        "    * 밀도 및 황산염과 양의 상관 관계(강력)\n",
        "- 염화물: \n",
        "    * 총 이산화황과 음의 상관관계(강력)\n",
        "- 유리 이산화황:\n",
        "    * 황산염과 양의 상관 관계(강력)\n",
        "- 밀도:\n",
        "    * 황산염과 양의 상관 관계(강력)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 히트맵 그림 크기 설정\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "# 상삼각행렬 마스크 생성\n",
        "mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n",
        "\n",
        "# 데이터프레임 상관계수 히트맵 생성\n",
        "sns.heatmap(df.corr(), annot=True, fmt='.2f', linewidths=2, cmap='jet', mask=mask)\n",
        "\n",
        "# x축 레이블 회전 각도 설정\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# 그림 출력\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Histogram plot of all features from the below figures**\n",
        "- Few of them:  normally distributed\n",
        "- Other: rightly skewed\n",
        "- The range of each feature: not huge.\n",
        "\n",
        "**아래 그림의 모든 형상에 대한 히스토그램 그림**\n",
        "- 그 중 일부: 정규 분포\n",
        "- 기타: 오른쪽으로 치우침\n",
        "- 각 기능의 범위: 크지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터프레임(df)의 모든 변수에 대해 히스토그램을 생성하는 파이썬 코드다. \n",
        "\n",
        "\n",
        "# 히스토그램 생성을 위한 서브플롯 그리드 설정\n",
        "fig, ax = plt.subplots(ncols=6, nrows=2, figsize=(20,10))\n",
        "\n",
        "# 히스토그램을 그릴 서브플롯 인덱스 초기화\n",
        "index = 0\n",
        "\n",
        "# 서브플롯 배열을 1차원 배열로 변환\n",
        "ax = ax.flatten()\n",
        "\n",
        "# 데이터프레임의 각 열에 대해 반복하여 히스토그램 생성\n",
        "for col, value in df.items():\n",
        "    # 해당 열의 히스토그램 생성 및 서브플롯에 그리기\n",
        "    sns.histplot(value, color='r', ax=ax[index], kde=True)\n",
        "    \n",
        "    # 서브플롯 인덱스 증가\n",
        "    index += 1\n",
        "\n",
        "# 전체 그림 레이아웃 조정\n",
        "plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusions after performing EDA**\n",
        "- Used the Wine Quality dataset provided by UCI to perform EDA\n",
        "- Discussed how we can perform EDA techniques such as data loading, data wrangling, correlation between variables\n",
        "- Found that the dataset has outliers and rightly skewed features\n",
        "- Future works: \n",
        "    * Remove outliers\n",
        "    * Apply feature scaling to the dataset\n",
        "    * Handle the class imbalance problem\n",
        "- ***Now, let focus on making a Bayesian neural network model to predict the wine quality score***\n",
        "\n",
        "**EDA 수행 후의 결론**\n",
        "탐색적 데이터 분석(Exploratory Data Analysis, EDA)\n",
        "- UCI에서 제공하는 와인 품질 데이터 세트를 사용하여 EDA를 수행\n",
        "- 데이터 로드, 데이터 논쟁, 변수 간의 상관 관계와 같은 EDA 기술을 수행하는 방법에 대해 논의했습니다\n",
        "- 데이터 집합에 특이치와 오른쪽으로 치우친 형상이 있음을 발견함\n",
        "- 향후 작업: \n",
        "    * 특이치 제거\n",
        "    * 데이터 세트에 피쳐 스케일링 적용\n",
        "    * 클래스 불균형 문제 처리\n",
        "- ***이제 와인 품질 점수를 예측하기 위한 베이지안 신경망 모델을 만드는 데 집중하겠습니다**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RDvvq2pHYoKu"
      },
      "source": [
        "## Create training and evaluation datasets\n",
        "- Construct `get_train_and_test_splits()` function to:\n",
        "    * Load the `wine_quality` dataset using `tfds.load()`\n",
        "    * Convert the target feature to float\n",
        "    * Shuffle the dataset and split it into training and test sets\n",
        "    * Take the first `train_size` examples as the train split, and the rest as the test split\n",
        "\n",
        "    ## 교육 및 평가 데이터셋 생성\n",
        "- 'get_train_and_test_splits()' 함수를 다음과 같이 구성합니다:\n",
        "    * 'tfds(tensorflow_datasets).load()'를 사용하여 'wine_quality' 데이터 세트를 로드합니다\n",
        "    * 대상 피쳐를 부동으로 변환\n",
        "    * 데이터 세트를 순서대로 섞고 교육 및 테스트 세트로 분할\n",
        "    * 첫 번째 'train_size' 에는 훈련 세트 분할로, 나머지는 테스트 세트를 위한 분할을 실행한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SUNUdimYoKu"
      },
      "outputs": [],
      "source": [
        "#  TensorFlow 데이터셋(tfds)의 \"wine_quality\" 데이터셋을 로드하고, 훈련 데이터셋과 테스트 데이터셋으로 분할하는 함수를 정의한 파이썬 코드\n",
        "\n",
        "\n",
        "# 훈련 및 테스트 데이터셋 분할 함수 정의\n",
        "def get_train_and_test_splits(train_size, batch_size=1):\n",
        "    # 데이터셋이 작고 메모리에 적재 가능하기 때문에 동일한 크기의 버퍼로 미리 가져오기(prefetch) 수행\n",
        "    dataset = (\n",
        "        tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\")\n",
        "        .map(lambda x, y: (x, tf.cast(y, tf.float32)))\n",
        "        .prefetch(buffer_size=dataset_size)\n",
        "        .cache()\n",
        "    )\n",
        "    # 데이터셋의 크기만큼 버퍼로 섞기(shuffle) 수행\n",
        "    train_dataset = (\n",
        "        dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
        "    )\n",
        "    # 훈련 데이터 이후 데이터를 테스트 데이터셋으로 사용\n",
        "    test_dataset = dataset.skip(train_size).batch(batch_size)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 해당 코드에서 알아두면 좋은 메서드(기능)\n",
        "\n",
        "##### .shuffle(buffer_size=train_size): 이 메서드는 데이터셋을 무작위로 섞습니다.\n",
        "buffer_size 매개변수는 섞기 전에 로드할 데이터의 버퍼 크기를 결정합니다. 버퍼 크기가 작을수록 섞이는 정도가 낮아지고, 더 크면 섞이는 정도가 높아집니다. 여기서는 train_size와 같게 설정하여 모든 데이터를 완전히 섞어줍니다. 하지만 메모리 문제가 없다면 일반적으로 buffer_size를 전체 데이터셋 크기와 같게 설정하는 것이 좋습니다.\n",
        "\n",
        "##### .batch(batch_size): 이 메서드는 데이터셋을 일정 크기의 배치로 나눕니다.\n",
        "batch_size 매개변수는 한 배치에 포함될 데이터 샘플의 수를 결정합니다. 이렇게 배치로 구성된 데이터셋은 모델 학습 시 가중치를 업데이트하는 데 사용됩니다. 작은 배치 크기는 더 빈번한 가중치 업데이트를 통해 더 빠르게 수렴할 수 있지만, 큰 배치 크기는 더 정확한 그래디언트 추정을 제공하고 메모리 효율성이 높습니다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_9m7k0RWYoKv"
      },
      "source": [
        "## Compile, train, and evaluate the model\n",
        "- NN architecture: 2 layers with 8 units each (can be tuned)\n",
        "- Learning rate: 0.001 (can be tuned)\n",
        "- Construct `run_experiment()` function to:\n",
        "    * Compile the model: `model.comnpile()`\n",
        "    * Train the model: `model.fit()`\n",
        "    * Evaluate the model on the train and test sets: `model.evaluate()`\n",
        "\n",
        "    ## 모델 컴파일, 교육 및 평가\n",
        "- NN 아키텍처: 각각 8개의 유닛이 있는 2개의 레이어(튜닝 가능)\n",
        "- 학습률: 0.001(조정 가능)\n",
        "- run_experiment()' 함수를 구성하여 다음을 수행합니다:\n",
        "    * 모델 'model.comnpile()'을 컴파일합니다\n",
        "    * 모델 교육: 'model.fit()'\n",
        "    * 열차 및 테스트 세트의 모델 평가: 'model.evaluate()'\n",
        "\n",
        "\n",
        "####  주어진 모델과 손실 함수를 사용하여 실험을 실행하는 함수를 정의한 파이썬 코드,  모델을 컴파일하고, 훈련 데이터셋을 사용하여 모델을 학습시키며, 훈련 및 테스트 데이터셋에 대한 모델 성능을 평가하는 실질적으로 논문 작성시 이행해야되는 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQnPQdrnYoKv"
      },
      "outputs": [],
      "source": [
        "# 은닉층 설정 (2개의 은닉층, 각각 8개의 유닛)\n",
        "hidden_units = [8, 8]\n",
        "\n",
        "# 학습률 설정\n",
        "learning_rate = 0.001\n",
        "\n",
        "# 실험 실행 함수 정의\n",
        "def run_experiment(model, loss, train_dataset, test_dataset):\n",
        "    # 모델 컴파일 (최적화기, 손실 함수, 평가 지표 설정)\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "        loss=loss,\n",
        "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "\n",
        "# 메시지 출력은 무시해도 좋음 버그나 오류가 났을때 어디에서 났는지 확인하기 위함\n",
        "\n",
        "    # 모델 훈련 시작 메시지 출력\n",
        "    print(\"Start training the model...\")\n",
        "\n",
        "    # 모델 훈련 수행 (훈련 데이터셋, 검증 데이터셋 사용)\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)\n",
        "\n",
        "    # 모델 훈련 완료 메시지 출력\n",
        "    print(\"Model training finished.\")\n",
        "\n",
        "    # 훈련 데이터셋을 사용한 모델 평가 (RMSE 계산)\n",
        "    _, rmse = model.evaluate(train_dataset, verbose=0)\n",
        "    print(f\"Train RMSE: {round(rmse, 3)}\")\n",
        "\n",
        "    # 모델 성능 평가 시작 메시지 출력\n",
        "    print(\"Evaluating model performance...\")\n",
        "\n",
        "    # 테스트 데이터셋을 사용한 모델 평가 (RMSE 계산)\n",
        "    _, rmse = model.evaluate(test_dataset, verbose=0)\n",
        "    print(f\"Test RMSE: {round(rmse, 3)}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4sZT7fZFYoKw"
      },
      "source": [
        "## Create model inputs (i.e., name the inputs)\n",
        "## 모델에 들어갈 입력값들 설정하기\n",
        "\n",
        "\n",
        "#### 주어진 특성 이름 목록에 대해 모델의 입력 레이어를 생성하는 함수를 정의한 파이썬 코드, 각 특성에 대해 입력 레이어를 생성하고, 이를 딕셔너리에 저장하여 반환한다.\n",
        "\n",
        "딕셔너리 란: 사전형 저장형식인데 사전이 낱말에 설명이 써있듯 유일한 key 값에 value 를 저장해놓는것을 뜻함. 프로그래밍에서 사전형 저장방식이 나중에 읽어올때 속도가 빠르기에 사용됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbdsi0eQYoKw"
      },
      "outputs": [],
      "source": [
        "# 특성 이름 목록 설정\n",
        "FEATURE_NAMES = [\n",
        "    \"fixed acidity\",\n",
        "    \"volatile acidity\",\n",
        "    \"citric acid\",\n",
        "    \"residual sugar\",\n",
        "    \"chlorides\",\n",
        "    \"free sulfur dioxide\",\n",
        "    \"total sulfur dioxide\",\n",
        "    \"density\",\n",
        "    \"pH\",\n",
        "    \"sulphates\",\n",
        "    \"alcohol\",\n",
        "]\n",
        "\n",
        "# 모델 입력 생성 함수 정의\n",
        "def create_model_inputs():\n",
        "    # 입력 딕셔너리 초기화\n",
        "    inputs = {}\n",
        "\n",
        "    # 각 특성에 대해 입력 레이어 생성 및 딕셔너리에 저장\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        inputs[feature_name] = layers.Input(\n",
        "            name=feature_name, shape=(1,), dtype=tf.float32\n",
        "        )\n",
        "    \n",
        "    # 생성된 입력 딕셔너리 반환\n",
        "    return inputs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p1yX8N2mYoKx"
      },
      "source": [
        "## Experiment 1: standard neural network\n",
        "- Construct `create_baseline_model()` function to create a standard deterministic neural network\n",
        "\n",
        "## 실험 1: 표준 신경망(SNN 기법)\n",
        "- 'create_baseline_model()' 함수를 구성하여 표준 결정론적 신경망(SDNN)을 만든다.\n",
        "\n",
        "기본 모델을 생성하는 함수를 정의한 파이썬 코드\n",
        "1. 먼저 입력 레이어를 생성\n",
        "2. 특성 벡터를 만들고\n",
        "3. 배치 정규화를 적용\n",
        "4. 결정적 가중치를 사용하여 은닉층을 생성\n",
        "5. 마지막으로 출력층을 생성하여 모델을 완성"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. 먼저 입력 레이어를 생성\n",
        "#### 2. 특성 벡터를 만들고\n",
        "이 단계에서는 각 특성(예: 고정 산도, 휘발성 산도 등)에 대한 입력 레이어를 하나의 특성 벡터로 결합한다.\n",
        "\n",
        "\n",
        "특성 벡터는 여러 개의 특성을 포함하는 데이터의 표현입니다. 이 예시에서는 와인 품질 데이터셋에서 각 와인 샘플은 11개의 특성으로 구성되어 있다. = 각 샘플에 대한 특성 벡터는 11차원 벡터이다. \n",
        "\n",
        "\n",
        "이 단계에서는 입력 레이어를 연결하여 신경망이 각 샘플에 대해 11차원 특성 벡터를 처리할 수 있도록 한다.\n",
        "#### 3. 배치 정규화를 적용\n",
        "신경망의 각 층에서 입력 분포를 정규화하는 과정\n",
        "\n",
        "\n",
        "이를 통해 가중치 업데이트가 더 효율적으로 이루어지며, 학습 속도가 향상되고 신경망이 초기 가중치 값에 덜 민감하게 되어, 학습 과정이 더 안정적으로 진행된다.\n",
        "\n",
        "\n",
        "배치 정규화는 각 층의 입력 데이터를 평균이 0이고 표준편차가 1인 분포로 변환한다. (정규화 관련 개념)\n",
        "\n",
        "\n",
        "#### 4. 결정적 가중치를 사용하여 은닉층을 생성\n",
        "*** 은닉층은 신경망의 핵심 구성 요소로, 입력 데이터에서 패턴을 학습하는 역할이다. 실질적으로 논문 작성시 중요한 부분 *** \n",
        "\n",
        "\n",
        "이 예제에서는 두 개의 완전 연결된(Dense) 은닉층을 사용하며, 각 은닉층에는 8개의 뉴런이 있다.\n",
        "\n",
        "\n",
        "위에 있었음\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "hidden_units 리스트의 각 요소에 대해 Dense 층이 생성되고, 각 층에는 해당 요소의 값(여기서는 8)만큼의 뉴런이 포함된다.\n",
        "\n",
        "\n",
        "각 뉴런은 주어진 특성 벡터에 대해 가중치를 학습하고, 시그모이드 활성화 함수(아래 개념 첨부하였음)를 사용하여 0과 1 사이의 값을 출력하며, 이를 통해 뉴런의 출력 값이 일정 범위 내에서 유지되게 한다.(안정적인 결과값 도출)\n",
        "#### 5. 마지막으로 출력층을 생성하여 모델을 완성\n",
        "\n",
        "출력층은 신경망이 학습한 패턴을 바탕으로 예측 값을 생성하는 역할을 한다.\n",
        "\n",
        "\n",
        " 이 예제에서는 회귀 문제를 해결하기 때문에, 출력층은 단일 뉴런을 가지며, 별도의 활성화 함수를 사용하지 않는다.\n",
        " \n",
        " \n",
        " 이 뉴런은 신경망의 최종 출력 값을 생성한다.\n",
        " \n",
        " \n",
        " *** 출력층의 목적은 신경망이 학습한 패턴을 사용하여 입력 데이터에 대한 연속적인 예측 값을 출력하는 것 = 우리가 논문에서 얻어야하는 결과값 ***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "시그모이드 활성화 함수는 신경망에서 많이 사용되는 비선형 활성화 함수 중 하나\n",
        "\n",
        "\n",
        "시그모이드 함수는 S자 형태를 가지며, 입력 값을 0과 1 사이의 출력 값으로 변환합니다. \n",
        "\n",
        "시그모이드 함수는 다음과 같은 수식으로 정의됩니다.\n",
        "\n",
        "\n",
        "σ(x) = 1 / (1 + exp(-x))\n",
        "\n",
        "\n",
        "여기서 x는 입력 값이고, exp(-x)는 e의 -x 거듭제곱입니다.\n",
        "\n",
        "\n",
        "\n",
        "시그모이드 활성화 함수는 다음과 같은 특징이 있습니다:\n",
        "\n",
        "\n",
        "\n",
        "출력 값의 범위가 0과 1 사이입니다. 이는 신경망의 출력 값이 특정 범위 내에서 유지되도록 도와줍니다.\n",
        "함수의 미분이 간단하며, 미분 값은 최대 0.25입니다. 이는 경사 하강법(Gradient Descent)과 같은 최적화 알고리즘에서 가중치 업데이트를 수월하게 합니다.\n",
        "시그모이드 함수는 입력 값이 0에 가까울 때 가장 높은 민감도를 가지며, 입력 값이 매우 크거나 작을 때는 민감도가 낮아집니다. 이로 인해 신경망이 큰 가중치 값을 학습하는 것을 방지하며, 일반화 능력을 향상시킵니다.\n",
        "\n",
        "\n",
        "그러나 시그모이드 함수는 몇 가지 단점도 있습니다:\n",
        "\n",
        "입력 값의 절댓값이 클 경우, 시그모이드 함수의 미분 값이 매우 작아져서 기울기 소실(Vanishing Gradient) 문제가 발생할 수 있습니다. 이 문제는 신경망이 깊어질수록 학습이 어려워지는 원인이 됩니다.\n",
        "시그모이드 함수는 출력 값의 평균이 0이 아닌 0.5로, 학습 과정에서 가중치 업데이트가 불균형할 수 있습니다.\n",
        "이러한 단점으로 인해, 현재는 ReLU(Rectified Linear Unit)와 같은 다른 활성화 함수들이 더 널리 사용되고 있습니다. 그러나 시그모이드 함수는 이진 분류 문제의 출력층에서 여전히 널리 사용되며, 확률 값을 출력하는 데 적합합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRwUJuSmYoKx"
      },
      "outputs": [],
      "source": [
        "# 기본 모델 생성 함수 정의\n",
        "def create_baseline_model():\n",
        "    # 입력 레이어 생성\n",
        "    inputs = create_model_inputs()\n",
        "    input_values = [value for _, value in sorted(inputs.items())]\n",
        "    \n",
        "    # 입력 레이어를 연결하여 특성 벡터 생성\n",
        "    features = keras.layers.concatenate(input_values)\n",
        "    \n",
        "    # 배치 정규화 적용\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # 은닉층 생성 (결정적 가중치 사용)\n",
        "    for units in hidden_units:\n",
        "        features = layers.Dense(units, activation=\"sigmoid\")(features)\n",
        "    \n",
        "    # 출력층 생성 (결정적 출력: 단일 점 추정)\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "\n",
        "    # 입력과 출력을 사용하여 모델 생성\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fCO4nl8KYoKy"
      },
      "source": [
        "- Split the wine dataset into training and test sets, with 85% and 15% of the examples, respectively\n",
        "\n",
        "주어진 데이터 셋을 훈련 케이스(트케) 테스트케이스(테케)로 나눈다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-A07uS0YoKy"
      },
      "outputs": [],
      "source": [
        "dataset_size = 4898\n",
        "batch_size = 256\n",
        "train_size = int(dataset_size * 0.85)\n",
        "train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qFbiDIvDYoKy"
      },
      "source": [
        "- Let's train the baseline model: \n",
        "    * `.save_weights()`: save the weights of the model\n",
        "    * `.load_weights()`: load the weights of the model\n",
        "- Use the `MeanSquaredError` as the loss function\n",
        "\n",
        "- 기본 모델을 교육해 보겠습니다: \n",
        "    * \".save_between\": 모델의 가중치를 저장\n",
        "    * ''.load_between'': 모델의 가중치 로드\n",
        "- '평균 제곱 오차'를 손실 함수로 사용합니다(이건 나중에 달라질수 있음 근데 본질적으로는 본래값하고 결과값이 얼마나 차이나나에 대한 것)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j41NurDNYoKy"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100  # 모델을 학습할 때 사용할 에폭(epoch) 수를 100으로 설정합니다.\n",
        "\n",
        "mse_loss = keras.losses.MeanSquaredError()  # 평균 제곱 오차(Mean Squared Error) 손실 함수를 사용합니다.\n",
        "\n",
        "baseline_model = create_baseline_model()  # 기본 모델을 생성합니다.\n",
        "\n",
        "# 모델 가중치가 저장된 파일이 있는지 확인합니다.\n",
        "if os.path.exists('pretrained_model/baseline_model.h5'):\n",
        "    print(\"Model exists, load weights\")  # 가중치 파일이 있으면, 파일에서 가중치를 불러옵니다.\n",
        "    baseline_model.load_weights('pretrained_model/baseline_model.h5')\n",
        "else:\n",
        "    print(\"Model not exists, train model\")  # 가중치 파일이 없으면, 모델을 학습시킵니다.\n",
        "    run_experiment(baseline_model, mse_loss, train_dataset, test_dataset)\n",
        "    baseline_model.save_weights('pretrained_model/baseline_model.h5')  # 학습된 모델의 가중치를 저장합니다.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5xuP1hkFYoKz"
      },
      "source": [
        "- Let's take a sample from the test set use the model to obtain predictions for them\n",
        "- Baseline model: deterministic\n",
        "    * A single a *point estimate* prediction for each test example\n",
        "    * No information about the uncertainty of the model nor the prediction\n",
        "\n",
        "\n",
        "    - 모형을 사용하여 검정 세트에서 표본을 추출하여 예측값을 구하겠습니다\n",
        "- 기준 모형: 결정론적 모형\n",
        "    * 각 검정 예제에 대한 단일 *점 추정치* 예측\n",
        "    https://en.wikipedia.org/wiki/Point_estimation\n",
        "    * 모형의 불확실성 또는 예측에 대한 정보가 없음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = 50  # 테스트 데이터셋에서 50개의 샘플을 사용합니다.\n",
        "\n",
        "# 테스트 데이터셋을 섞고(shuffle), 50개의 샘플을 뽑습니다.\n",
        "examples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(sample))[0]\n",
        "\n",
        "predicted = baseline_model(examples).numpy()  # 기본 모델을 사용하여 예측값을 계산합니다.\n",
        "\n",
        "# 샘플 중 처음 10개에 대한 예측값과 실제값을 출력합니다.\n",
        "for idx in range(sample):\n",
        "    if idx == 10:\n",
        "        break\n",
        "    print(f\"Predicted: {round(float(predicted[idx][0]), 1)} - Actual: {targets[idx]}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Visualize the correlation between the predicted and actual wine quality scores\n",
        "\n",
        "시각화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgDXlhqnYoKz"
      },
      "outputs": [],
      "source": [
        "# 실제 값(targets)을 NumPy 배열로 변환합니다.\n",
        "y_true = np.asarray(targets.numpy()).reshape(-1)\n",
        "y_pred = predicted.reshape(-1)\n",
        "\n",
        "# 상관 계수를 계산합니다.\n",
        "corr_coef = np.corrcoef(y_pred, y_true)[0, 1]\n",
        "\n",
        "# 예측 값과 실제 값을 산점도로 그립니다. \n",
        "# \n",
        "\n",
        "# 시각화 하는것임\n",
        "plt.scatter(y_true, y_pred)\n",
        "plt.ylabel(\"Predicted\")  # y축 레이블을 설정합니다.\n",
        "plt.xlabel(\"Actual\")  # x축 레이블을 설정합니다.\n",
        "plt.title(f\"Predicted vs Actual (Correlation: {corr_coef:.2f})\")  # 그래프 제목을 설정합니다.\n",
        "plt.show()  # 그래프를 출력합니다.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "toTpjjJ-YoKz"
      },
      "source": [
        "## Experiment 2: Bayesian neural network (BNN)\n",
        "\n",
        "The object of the Bayesian approach:\n",
        "-  Capture the *epistemic uncertainty*, which is uncertainty about the model fitness, due to limited training data\n",
        "\n",
        "Main idea:\n",
        "-  Instead of learning specific weight (and bias) *values* in the neural network, the Bayesian approach learns weight *distributions* from which we can sample to produce an output for a given input to encode weight uncertainty\n",
        "\n",
        "***Thus, define prior and the posterior distributions of these weights, and the training process is to learn the parameters of these distributions***\n",
        "\n",
        "## 실험 2: 베이지안 신경망(BNN)\n",
        "\n",
        "베이지안 접근법의 목표는 다음과 같습니다:\n",
        "-  제한된 교육 데이터로 인해 모델 적합성에 대한 불확실성인 *통계학적 불확실성*을 포착합니다\n",
        "\n",
        "주요 아이디어:\n",
        "-  신경망에서 특정 가중치(및 편향) *값*을 학습하는 대신, 베이지안 접근 방식은 가중치 *분포*를 학습하여 가중치 불확실성을 인코딩하기 위해 주어진 입력에 대한 출력을 생성할 수 있습니다\n",
        "\n",
        "\n",
        "위의 ANN, SNN 은 값을 통한 학습이지만 이건 분포를 통한 학습. 해당 깃 내에 있는 introduction 을 확인해보길 바람\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "***그러므로 이러한 가중치의 사전 및 사후 분포를 정의하고, 교육 과정은 이러한 분포의 매개 변수를 학습하는 것입니다.**\n",
        "\n",
        "\n",
        "이 함수는 다변수 가우시안 분포를 반환하며, 이 분포의 학습 가능한 매개변수는 평균, 분산, 공분산입니다. 사후 분포는 베이지안 신경망의 학습 과정에서 업데이트되어 가중치에 대한 불확실성을 추정한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUINMRQKYoK0"
      },
      "outputs": [],
      "source": [
        "# 사전 가중치 분포를 평균이 0이고 표준편차가 1인 정규 분포로 정의합니다.\n",
        "# 이 예제에서 사전 분포의 매개변수는 고정되어 있어 학습 가능하지 않습니다.\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    prior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.DistributionLambda(\n",
        "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
        "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return prior_model\n",
        "\n",
        "\n",
        "# 변분 사후 가중치 분포를 다변수 가우시안으로 정의합니다.\n",
        "# 이 분포의 학습 가능한 매개변수는 평균, 분산, 공분산입니다.\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    posterior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.VariableLayer(\n",
        "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
        "            ),\n",
        "            tfp.layers.MultivariateNormalTriL(n),\n",
        "        ]\n",
        "    )\n",
        "    return posterior_model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zs2u0YJ7YoK0"
      },
      "source": [
        "- Use the `tfp.layers.DenseVariational` layer instead of the standard `keras.layers.Dense` layer in the neural network model\n",
        "\n",
        "tfp 이거 위에서 import tensorflow_probability as tfp tensorflow_probability를 임포트 한것이고 그 라이브러리 내에 있는 Dense 를 사용하는 모델을 불러와서 사용한다는 것임."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1a4KupWYoK0"
      },
      "outputs": [],
      "source": [
        "# 베이지안 신경망(BNN) 모델을 생성하는 함수를 정의 입력 특성에 대한 불확실성을 고려하여 가중치를 추정\n",
        "# create_bnn_model 함수는 학습 데이터셋의 크기(train_size)를 인수로 받고 있다.\n",
        "\n",
        "\n",
        "def create_bnn_model(train_size):\n",
        "    # 입력 레이어 생성\n",
        "    inputs = create_model_inputs()\n",
        "\n",
        "    # 입력 레이어를 연결하여 특성 벡터 생성\n",
        "    features = keras.layers.concatenate(list(inputs.values()))\n",
        "\n",
        "    # 배치 정규화 적용\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # create_model_inputs 함수를 호출하여 입력 레이어를 생성하고, concatenate를 사용하여 입력 레이어를 연결하여 특성 벡터를 생성한다.\n",
        "\n",
        "    # 가중치 불확실성이 있는 은닉층 생성 (DenseVariational 레이어 사용)\n",
        "    for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / train_size,\n",
        "            activation=\"sigmoid\",\n",
        "        )(features)\n",
        "\n",
        "    # hidden_units 리스트에 있는 각 원소를 사용하여 은닉층을 생성\n",
        "    # 여기서는 가중치에 대한 불확실성을 고려하기 위해 DenseVariational 레이어를 사용\n",
        "    # 이 레이어는 사전 분포 함수(make_prior_fn=prior)와 사후 분포 함수(make_posterior_fn=posterior)를 인수로 받고\n",
        "    # Kullback-Leibler 발산(KL divergence)에 대한 가중치(kl_weight=1 / train_size)를 설정하여 모델의 복잡성을 제어한다. \n",
        "    \n",
        "    # 활성화 함수로 시그모이드를 사용\n",
        "    # 출력층 생성 (결정적 출력: 단일 점 추정)\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "\n",
        "    # 입력과 출력을 사용하여 모델 생성 = 베이지안 모델 = 논문의 결과물에 필요한 모델 = 이 과정을 반복\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F5H5A_klYoK1"
      },
      "source": [
        "The epistemic uncertainty:\n",
        "- Can be reduced as we increase the size of the training data\n",
        "- The more data the BNN model sees, the more it is certain about its estimates for the weights (distribution parameters)\n",
        "\n",
        "**Let's test this behaviour by training the BNN model on a small subset of the training set, and then on the full training set, to compare the output variances**\n",
        "\n",
        "인식론적 불확실성:\n",
        "- 교육 데이터의 크기를 늘리면 줄일 수 있음 = 데이터 많을수록 불확실성이 줄어진다 = 당연한 소리\n",
        "- BNN 모델이 더 많은 데이터를 볼수록 가중치(분포 모수)에 대한 추정치가 더 확실해집니다\n",
        "\n",
        "**BNN 모델을 교육 세트의 작은 하위 세트에서 교육한 다음 전체 교육 세트에서 출력 분산을 비교하여 이 동작을 테스트합니다**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9AAafpTFYoK1"
      },
      "source": [
        "### Train BNN  with a small training subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74cnvoe_YoK1"
      },
      "outputs": [],
      "source": [
        "# 베이지안 신경망(BNN) 모델을 학습시키는 과정\n",
        "# 총 500 num_epochs = 500 동안 모델을 학습시키며, 학습 데이터셋의 30% 크기인 작은 데이터셋(small_train_dataset)을 사용한다.\n",
        "\n",
        "num_epochs = 500\n",
        "train_sample_size = int(train_size * 0.3)\n",
        "small_train_dataset = train_dataset.unbatch().take(train_sample_size).batch(batch_size)\n",
        "\n",
        "# 전체 학습 데이터셋 크기의 30%에 해당하는 작은 데이터셋을 생성\n",
        "bnn_model_small = create_bnn_model(train_sample_size)\n",
        "\n",
        "# 작은 데이터셋의 크기를 인자로 넘겨주며 베이지안 신경망 모델을 생성\n",
        "# 이미 학습된 모델이 있는 경우, 해당 모델의 가중치를 불러옵니다. 그렇지 않은 경우, 작은 데이터셋을 사용하여 모델을 학습시키고, 학습된 가중치를 저장한다.\n",
        "\n",
        "# 모델이 더 적은 데이터로도 학습할 수 있는지 확인하는 과정이다.\n",
        "if os.path.exists('pretrained_model/bnn_model_full.h5'):\n",
        "    print(\"Model exists, load weights\")\n",
        "    bnn_model_small.load_weights('pretrained_model/bnn_model_small.h5')\n",
        "else:\n",
        "    print(\"Model not exists, train model\")\n",
        "    run_experiment(bnn_model_small, mse_loss, train_dataset, test_dataset)\n",
        "    bnn_model_small.save_weights('pretrained_model/bnn_model_small.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "otbloUyQYoK1"
      },
      "source": [
        "- The trained a BNN model:\n",
        "    * Produce a different output each time we call it with the same input, since each time a new set of weights are sampled from the distributions to construct the network and produce an output\n",
        "    * The less certain the mode weights are, the more variability (wider range) we will see in the outputs of the same inputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- 훈련된 BNN 모델:\n",
        "    * 네트워크를 구성하고 출력을 생성하기 위해 분포에서 새로운 가중치 집합을 샘플링할 때마다 동일한 입력으로 호출할 때마다 다른 출력을 생성합니다\n",
        "    * 모드 가중치가 불확실할수록 동일한 입력의 출력에서 더 많은 가변성(넓은 범위)을 확인할 수 있습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYXLd33NYoK2"
      },
      "outputs": [],
      "source": [
        "#  베이지안 신경망 모델(BNN)을 사용하여 예측을 수행하고, 예측값의 평균, 최소값, 최대값 및 범위를 계산한다. \n",
        "# 여기서 주어진 iterations는 예측값을 얻기 위해 모델을 실행할 횟수를 나타냅니다(기본값은 100회).\n",
        "\n",
        "def compute_predictions(model, iterations=100):\n",
        "    predicted = []\n",
        "    for _ in range(iterations):\n",
        "        predicted.append(model(examples).numpy())\n",
        "    predicted = np.concatenate(predicted, axis=1)\n",
        "\n",
        "    # iterations 만큼의 예측값을 얻기 위해 모델을 여러 번 실행한 뒤, 각 실행 결과로 얻은 예측값을 predicted 목록에 추가한다.\n",
        "    # 그 다음, predicted 목록에 있는 모든 예측값을 하나의 NumPy(큰 숫자 다루는 라이브러리) 배열로 연결\n",
        "\n",
        "    prediction_mean = np.mean(predicted, axis=1).tolist()\n",
        "    prediction_min = np.min(predicted, axis=1).tolist()\n",
        "    prediction_max = np.max(predicted, axis=1).tolist()\n",
        "    prediction_range = (np.max(predicted, axis=1) - np.min(predicted, axis=1)).tolist()\n",
        "\n",
        "    # 예측값의 평균, 최소값, 최대값 및 범위를 계산\n",
        "    for idx in range(sample):\n",
        "        print(\n",
        "            f\"Predictions mean: {round(prediction_mean[idx], 2)}, \"\n",
        "            f\"min: {round(prediction_min[idx], 2)}, \"\n",
        "            f\"max: {round(prediction_max[idx], 2)}, \"\n",
        "            f\"range: {round(prediction_range[idx], 2)} - \"\n",
        "            f\"Actual: {targets[idx]}\"\n",
        "        )\n",
        "\n",
        "        \n",
        "    return prediction_mean, prediction_min, prediction_max, prediction_range\n",
        "\n",
        "# 각 샘플에 대해 예측값의 평균, 최소값, 최대값 및 범위를 출력하고 실제값과 비교 (compute_predictions를 이용해서)한다.\n",
        "predicted_mean, predicted_min,  predicted_max, predicted_range = compute_predictions(bnn_model_small)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Visualize the correlation between the predicted and actual wine quality scores with uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the errors\n",
        "errors = [(predicted_mean[i] - predicted_min[i], predicted_max[i] - predicted_mean[i]) for i in range(len(predicted_mean))]\n",
        "errors = np.asarray(errors).reshape(2, -1)\n",
        "\n",
        "# Create the scatter plot with error bars\n",
        "fig, ax = plt.subplots()\n",
        "# ax.errorbar(y_true, predicted_mean, yerr=errors, fmt='o', ecolor='lightgray', elinewidth=3, capsize=0)\n",
        "ax.errorbar(y_true, predicted_mean, yerr=errors, fmt='o',  ecolor='r', markersize=5, capsize=5)\n",
        "ax.set_ylabel('Predicted Wine Quality Scores')\n",
        "ax.set_xlabel('Actual Wine Quality Scores')\n",
        "ax.set_title('Correlation between Predicted and Actual Wine Quality Scores with Uncertainty')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RV0T2LLQYoK2"
      },
      "source": [
        "### Train BNN  with the whole training set.\n",
        "\n",
        "### 전체 트케로 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvXGTcPtYoK2"
      },
      "outputs": [],
      "source": [
        "# 먼저, num_epochs를 500으로 설정하고, 전체 학습 데이터셋 크기를 사용하여 BNN 모델을 생성\n",
        "num_epochs = 500\n",
        "bnn_model_full = create_bnn_model(train_size)\n",
        "\n",
        "# 이전에 학습된 모델 가중치가 있는 경우 해당 가중치를 로드하고, 그렇지 않은 경우 모델을 학습시키고 가중치를 저장\n",
        "if os.path.exists('pretrained_model/bnn_model_full.h5'):\n",
        "    print(\"Model exists, load weights\")\n",
        "    bnn_model_full.load_weights('pretrained_model/bnn_model_full.h5')\n",
        "else:\n",
        "    print(\"Model not exists, train model\")\n",
        "    run_experiment(bnn_model_full, mse_loss, train_dataset, test_dataset)\n",
        "    bnn_model_full.save_weights('pretrained_model/bnn_model_full.h5')\n",
        "\n",
        "# 학습된 BNN 모델을 사용하여 예측을 수행하고 결과를 저장\n",
        "predicted_mean, predicted_min,  predicted_max, predicted_range = compute_predictions(bnn_model_full)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9PGfHsiKYoK2"
      },
      "source": [
        "Notice that the model trained with the full training dataset shows smaller range\n",
        "(uncertainty) in the prediction values for the same inputs, compared to the model\n",
        "trained with a subset of the training dataset.\n",
        "\n",
        "\n",
        "\n",
        "전체 교육 데이터 세트로 훈련된 모델은 더 작은 범위를 보여줍니다\n",
        "모형과 비교하여 동일한 입력에 대한 예측 값에서 (반복성)\n",
        "교육 데이터 세트의 하위 집합으로 교육을 받았다(처음에는 작은 규모의 트케로 트레이닝했다)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Visualize the correlation between the predicted and actual wine quality scores with uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 예측된 와인 품질 점수와 실제 와인 품질 점수 간의 상관 관계를 그래프로 나타내고, 예측의 불확실성을 에러로 표시함\n",
        "errors = [(predicted_mean[i] - predicted_min[i], predicted_max[i] - predicted_mean[i]) for i in range(len(predicted_mean))]\n",
        "errors = np.asarray(errors).reshape(2, -1)\n",
        "\n",
        "#  예측된 평균, 최소 및 최대 값에서 에러 범위를 계산\n",
        "fig, ax = plt.subplots()\n",
        "# ax.errorbar(y_true, predicted_mean, yerr=errors, fmt='o', ecolor='lightgray', elinewidth=3, capsize=0)\n",
        "\n",
        "# plt.subplots() 함수를 사용하여 새로운 그림과 축을 생성합니다.\n",
        "# ax.errorbar() 함수를 사용하여 실제 와인 품질 점수와 예측된 평균 점수 사이의 관계를 나타내는 산점도를 그리고, 에러 바를 빨간색으로 표시\n",
        "ax.errorbar(y_true, predicted_mean, yerr=errors, fmt='o',  ecolor='r', markersize=5, capsize=5)\n",
        "ax.set_ylabel('Predicted Wine Quality Scores')\n",
        "ax.set_xlabel('Actual Wine Quality Scores')\n",
        "ax.set_title('Correlation between Predicted and Actual Wine Quality Scores with Uncertainty')\n",
        "\n",
        "# x축과 y축에 레이블을 추가하고 그래프의 제목을 설정한 후 plt.show()를 호출하여 그래프를 출력한다.\n",
        "# 이 그래프는 예측된 와인 품질 점수와 실제 와인 품질 점수 간의 상관 관계와 불확실성을 시각적으로 보여준다.\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XW5izvHZYoK3"
      },
      "source": [
        "## Experiment 3: probabilistic Bayesian neural network\n",
        "\n",
        "- So far, the output of the standard and the Bayesian NN models:\n",
        "    * Deterministic: produces a point estimate as a prediction for a given example\n",
        "\n",
        "-> To create a probabilistic NN, let's the model output a distribution\n",
        "\n",
        "-> Captures the *aleatoric uncertainty* as well, which is due to irreducible noise in the data, or to the stochastic nature of the process generating the data\n",
        "\n",
        "- In this example: \n",
        "    * For regression task: model the output as a `IndependentNormal` distribution, with learnable mean and variance parameters.\n",
        "    * For classification (binary classes): model the output as a `IndependentBernoulli`\n",
        "    * For classification (multiple classes): model the output as a `OneHotCategorical`\n",
        "\n",
        "\n",
        "## 실험 3: 확률론적 베이지안 신경망\n",
        "\n",
        "- 지금까지 표준 및 베이지안 NN 모델의 출력은 다음과 같습니다:\n",
        "    * 결정론적: 주어진 예제에 대한 예측으로 점 추정치를 생성합니다\n",
        "\n",
        "-> 확률론적 NN을 생성하기 위해, 모델이 분포를 출력합니다\n",
        "\n",
        "-> 데이터의 환원 불가능한 노이즈 또는 데이터를 생성하는 프로세스의 확률적 특성으로 인해 발생하는 *알레알릭 불확실성*도 캡처합니다\n",
        "\n",
        "- 이 예제에서는 다음을 수행합니다: \n",
        "    * 회귀 작업의 경우: 학습 가능한 평균 및 분산 매개 변수를 사용하여 출력을 '독립 정규' 분포로 모델링합니다.\n",
        "    * 분류(이진 클래스)의 경우: 출력을 '독립 베르누이'로 모델링\n",
        "    * 분류(복수 클래스)의 경우: 출력을 'OneHotCategorical'로 모델링\n",
        "\n",
        "\n",
        "### 뭔가 많아보이지만 걱정하지 않아도 되는 것이 더 진보된 모델(방식)을 적용하는 것 뿐이다. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpsLv_9vYoK3"
      },
      "outputs": [],
      "source": [
        "#  train_size를 인수로 받아 확률론적 BNN 모델을 생성하는 함수를 정의\n",
        "# 이 모델은 가중치의 불확실성을 고려하여 예측을 생성하는 데 사용\n",
        "\n",
        "def create_probablistic_bnn_model(train_size):\n",
        "    inputs = create_model_inputs()\n",
        "    features = keras.layers.concatenate(list(inputs.values()))\n",
        "    \n",
        "    # 이후에 배치 정규화를 적용한다.\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    \n",
        "    \n",
        "    # 은닉층은 tfp.layers.DenseVariational 레이어를 사용하여 구성되며, 이 레이어는 가중치의 불확실성을 고려한다.\n",
        "    # 이전의 예제와 마찬가지로 시그모이드 활성화 함수를 사용한다.\n",
        "    for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / train_size,\n",
        "            activation=\"sigmoid\",\n",
        "        )(features)\n",
        "\n",
        "\n",
        "\n",
        "    # 출력 레이어는 정규 분포의 평균과 분산을 학습하기 위해 layers.Dense 레이어를 사용하여 확률론적 출력을 생성하고\n",
        "    # tfp.layers.IndependentNormal 레이어를 사용하여 배치의 각 예제에 대한 독립적인 정규 분포를 생성\n",
        "    distribution_params = layers.Dense(units=2)(features)\n",
        "    outputs = tfp.layers.IndependentNormal(1)(distribution_params)\n",
        "\n",
        "    # 입력과 출력을 사용하여 Keras 모델을 생성하고 반환\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t4REEdjsYoK3"
      },
      "source": [
        "- Use the [negative loglikelihood](https://en.wikipedia.org/wiki/Likelihood_function) as loss function due to the output of the model is a distribution, rather than a point estimate:\n",
        "    *  Compute how likely to see the true data (targets) from the\n",
        "estimated distribution produced by the model.\n",
        "\n",
        "- 모형의 출력이 점 추정치가 아닌 분포이기 때문에 손실 함수로 [역로그 우도](https://en.wikipedia.org/wiki/Likelihood_function) 를 사용합니다: (관련 공부 필요할듯)\n",
        "    *  에서 실제 데이터(대상)를 볼 가능성을 계산합니다\n",
        "모형에 의해 생성된 추정된 분포."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z7nAYMzYoK3"
      },
      "outputs": [],
      "source": [
        "# 확률론적 BNN 모델을 훈련하고 저장하는 과정\n",
        "\n",
        "\n",
        "# 확률론적 BNN 모델을 훈련하기 위해 손실 함수로 사용되는 negative log-likelihood를 정의\n",
        "# 타겟 값과 예측된 확률 분포를 받아들이고, 확률 분포의 log 확률의 부정 값을 반환\n",
        "def negative_loglikelihood(targets, estimated_distribution):\n",
        "    return -estimated_distribution.log_prob(targets)\n",
        "\n",
        "# num_epochs를 1000으로 설정한 후, create_probablistic_bnn_model() 함수를 사용하여 확률론적 BNN 모델을 생성\n",
        "num_epochs = 1000\n",
        "prob_bnn_model = create_probablistic_bnn_model(train_size)\n",
        "\n",
        "# 사전 훈련된 모델의 가중치가 있는지 확인\n",
        "# 만약 사전 훈련된 모델의 가중치가 있다면, 이를 확률론적 BNN 모델에 로드한다.\n",
        "if os.path.exists('pretrained_model/prob_bnn_model.h5'):\n",
        "    print(\"Model exists, load weights\")\n",
        "    prob_bnn_model.load_weights('pretrained_model/prob_bnn_model.h5')\n",
        "# 사전 훈련된 모델의 가중치가 있는지 확인합니다. 만약 사전 훈련된 모델의 가중치가 있다면, 이를 확률론적 BNN 모델에 로드\n",
        "else:\n",
        "    print(\"Model not exists, train model\")\n",
        "    run_experiment(prob_bnn_model, negative_loglikelihood, train_dataset, test_dataset)\n",
        "    # 훈련이 완료되면, 훈련된 모델의 가중치를 저장\n",
        "    prob_bnn_model.save_weights('pretrained_model/prob_bnn_model.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VbTQ4JJoYoK4"
      },
      "source": [
        "Now let's produce an output from the model given the test examples.\n",
        "The output is now a distribution, and we can use its mean and variance\n",
        "to compute the confidence intervals (CI) of the prediction.\n",
        "\n",
        "\n",
        "이제 테스트 예제가 주어진 모델에서 출력을 생성해 보겠습니다.\n",
        "이제 출력은 분포이며, 평균과 분산을 사용할 수 있습니다\n",
        "예측의 신뢰 구간(CI)을 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2itDftEYoK4"
      },
      "outputs": [],
      "source": [
        "#  확률론적 BNN 모델을 사용하여 예측한 결과와 실제 값 사이의 비교를 출력\n",
        "prediction_distribution = prob_bnn_model(examples)\n",
        "\n",
        "# 테스트 데이터셋의 예제를 사용하여 확률론적 BNN 모델로 예측된 분포를 계산\n",
        "prediction_mean = prediction_distribution.mean().numpy().tolist()\n",
        "prediction_stdv = prediction_distribution.stddev().numpy()\n",
        "\n",
        "# The 95% CI is computed as mean ± (1.96 * stdv)\n",
        "#  예측된 분포의 평균과 표준 편차를 계산\n",
        "upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
        "lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
        "prediction_stdv = prediction_stdv.tolist()\n",
        "\n",
        "\n",
        "# 95% 신뢰 구간 (CI)를 계산\n",
        "# 평균에 ±1.96 * 표준 편차를 더하고 빼서 구간의 상한과 하한을 찾는다.\n",
        "for idx in range(sample):\n",
        "    # 각 예제에 대해 예측 평균, 표준 편차, 95% 신뢰 구간, 그리고 실제 값을 출력한다.\n",
        "    print(\n",
        "        f\"Prediction mean: {round(prediction_mean[idx][0], 2)}, \"\n",
        "        f\"stddev: {round(prediction_stdv[idx][0], 2)}, \"\n",
        "        f\"95% CI: [{round(lower[idx][0], 2)} - {round(upper[idx][0], 2)}]\"\n",
        "        f\" - Actual: {targets[idx]}\"\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Visualize the correlation between the predicted and actual wine quality scores with uncertainty\n",
        "\n",
        "- 예측된 와인 품질 점수와 실제 와인 품질 점수 사이의 상관 관계를 불확실성과 함께 시각화합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  확률론적 BNN 모델의 예측과 실제 값 사이의 관계를 시각화하고, 예측의 불확실성을 나타내는 오차 막대를 추가하는 코드\n",
        "\n",
        "prediction_mean = np.array(prediction_mean).reshape(-1)\n",
        "\n",
        "# 예측 평균을 Numpy 배열로 변환하고, 적절한 형태로 재구성\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# 새로운 그림과 축을 생성 에러와 함께 실제 값과 예측 평균을 기준으로 오차 막대 그래프를 생성한다.\n",
        "\n",
        "# yerr는 95% 신뢰 구간을 나타내는 오차 막대의 크기를 결정하며, 표준 편차에 1.96을 곱하여 계산\n",
        "# 오차 막대의 색상은 빨간색이며, 마커 크기는 5, 오차 막대 끝의 캡 크기는 5로 설정\n",
        "ax.errorbar(y_true, prediction_mean, yerr=np.array(prediction_stdv).reshape(1, -1) * 1.96, fmt='o',  ecolor='r', markersize=5, capsize=5)\n",
        "\n",
        "# 축 레이블과 그래프 제목을 설정\n",
        "ax.set_ylabel('Predicted Wine Quality Scores')\n",
        "ax.set_xlabel('Actual Wine Quality Scores')\n",
        "ax.set_title('Correlation between Predicted and Actual Wine Quality Scores with Uncertainty')\n",
        "\n",
        "# 그래프를 출력\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bayesian_neural_networks_wine",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
